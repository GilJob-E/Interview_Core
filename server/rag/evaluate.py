"""
RAG ì‹¤íš¨ì„± í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
Retrieval Qualityì™€ Generation Qualityë¥¼ ì¸¡ì •í•˜ì—¬ RAG ì‹œìŠ¤í…œì˜ íš¨ê³¼ë¥¼ ê²€ì¦
"""
import asyncio
import json
import time
import random
from pathlib import Path
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime

from dotenv import load_dotenv
load_dotenv()

# RAG ì‹œìŠ¤í…œ import
from rag import RAGSystem, index_exists
from rag.document_loader import load_interview_documents
from rag.chain import create_no_rag_chain


@dataclass
class RetrievalMetrics:
    """ê²€ìƒ‰ í’ˆì§ˆ ì§€í‘œ"""
    query: str
    expected_occupation: str
    expected_experience: str
    retrieved_docs: List[Dict]

    # Metrics
    occupation_match_rate: float  # ì§ì—…êµ° ì¼ì¹˜ìœ¨
    experience_match_rate: float  # ê²½ë ¥ ì¼ì¹˜ìœ¨
    avg_similarity_score: float   # í‰ê·  ìœ ì‚¬ë„ (ì¶”ì •)
    retrieval_time_ms: float      # ê²€ìƒ‰ ì‹œê°„


@dataclass
class GenerationMetrics:
    """ìƒì„± í’ˆì§ˆ ì§€í‘œ"""
    query: str
    response: str
    response_length: int
    generation_time_ms: float
    is_korean: bool              # í•œê¸€ ì‘ë‹µ ì—¬ë¶€
    is_question_format: bool     # ì§ˆë¬¸ í˜•íƒœì¸ì§€
    has_context_reference: bool  # ì»¨í…ìŠ¤íŠ¸ ì°¸ì¡° ì—¬ë¶€


@dataclass
class ComparisonMetrics:
    """RAG vs non-RAG ë¹„êµ ì§€í‘œ"""
    query: str

    # RAG ì‘ë‹µ
    rag_response: str
    rag_time_ms: float
    rag_is_question: bool
    rag_has_context: bool
    rag_specificity_score: float  # êµ¬ì²´ì„± ì ìˆ˜ (0~1)

    # non-RAG ì‘ë‹µ
    no_rag_response: str
    no_rag_time_ms: float
    no_rag_is_question: bool
    no_rag_has_context: bool
    no_rag_specificity_score: float

    # ë¹„êµ ê²°ê³¼
    quality_improvement: str  # "better", "same", "worse"
    improvement_score: float  # RAG ì ìˆ˜ - non-RAG ì ìˆ˜


@dataclass
class HybridComparisonMetrics:
    """Hybrid RAG ë¹„êµ ì§€í‘œ (ContextScorer ê¸°ë°˜)"""
    query: str

    # RAG ì‘ë‹µ
    rag_response: str
    rag_time_ms: float
    rag_context_score: float       # ContextScorer ì ìˆ˜ (0~1)
    rag_token_overlap: float       # í† í° ì˜¤ë²„ë© (0~1)
    rag_doc_reference: float       # ë¬¸ì„œ ì°¸ì¡°ìœ¨ (0~1)
    rag_semantic_similarity: float # ì˜ë¯¸ì  ìœ ì‚¬ë„ (0~1)
    rag_referenced_doc_count: int  # ì°¸ì¡°ëœ ë¬¸ì„œ ìˆ˜

    # non-RAG ì‘ë‹µ
    no_rag_response: str
    no_rag_time_ms: float

    # ë¹„êµ ê²°ê³¼
    selected_source: str          # "RAG" ë˜ëŠ” "non-RAG"
    context_threshold: float      # ì ìš©ëœ ì„ê³„ê°’

    # ê²€ìƒ‰ëœ ë¬¸ì„œ ì •ë³´
    retrieved_docs: List[Dict]


@dataclass
class EvaluationResult:
    """ì „ì²´ í‰ê°€ ê²°ê³¼"""
    timestamp: str
    total_queries: int

    # Retrieval Quality
    avg_occupation_match: float
    avg_experience_match: float
    avg_retrieval_time_ms: float

    # Generation Quality
    avg_response_length: float
    avg_generation_time_ms: float
    korean_rate: float
    question_format_rate: float

    # Details
    retrieval_results: List[Dict]
    generation_results: List[Dict]


class RAGEvaluator:
    """RAG ì‹œìŠ¤í…œ í‰ê°€ê¸°"""

    def __init__(self, rag_system: Optional[RAGSystem] = None):
        """
        í‰ê°€ê¸° ì´ˆê¸°í™”

        Args:
            rag_system: í‰ê°€í•  RAG ì‹œìŠ¤í…œ (Noneì´ë©´ ìƒˆë¡œ ìƒì„±)
        """
        if rag_system is None:
            if not index_exists():
                raise FileNotFoundError(
                    "Vector store index not found. "
                    "Run 'python -m rag.build_index' first."
                )
            self.rag = RAGSystem()
        else:
            self.rag = rag_system

    def create_test_queries(self, n_samples: int = 20) -> List[Dict]:
        """
        í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ìƒì„±

        ë‹¤ì–‘í•œ ë©´ì ‘ ë‹µë³€ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì‹œë®¬ë ˆì´ì…˜
        RAGê°€ íš¨ê³¼ì ìœ¼ë¡œ ì‘ë™í•˜ë„ë¡ 100~200ì ì´ìƒì˜ êµ¬ì²´ì ì¸ ì¿¼ë¦¬ ì‚¬ìš©
        """
        test_cases = [
            # ICT ê²½ë ¥
            {
                "query": "ì €ëŠ” 5ë…„ê°„ ë°±ì—”ë“œ ê°œë°œìë¡œ ì¼í•˜ë©´ì„œ Javaì™€ Spring Frameworkë¥¼ ì£¼ë¡œ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤. íŠ¹íˆ ìµœê·¼ 2ë…„ê°„ì€ ê¸°ì¡´ ëª¨ë†€ë¦¬ì‹ ì‹œìŠ¤í…œì„ MSAë¡œ ì „í™˜í•˜ëŠ” í”„ë¡œì íŠ¸ë¥¼ ë¦¬ë“œí–ˆëŠ”ë°, Spring Cloudì™€ Kubernetesë¥¼ í™œìš©í•´ì„œ 15ê°œì˜ ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ë¡œ ë¶„ë¦¬í–ˆê³ , API Gateway ì„¤ê³„ì™€ ì„œë¹„ìŠ¤ ê°„ í†µì‹  íŒ¨í„´ì„ ì •ë¦½í•´ì„œ ë°°í¬ ì‹œê°„ì„ 2ì‹œê°„ì—ì„œ 15ë¶„ìœ¼ë¡œ ë‹¨ì¶•ì‹œì¼°ìŠµë‹ˆë‹¤.",
                "expected_occupation": "ICT",
                "expected_experience": "EXPERIENCED"
            },
            {
                "query": "ë°ì´í„° ì—”ì§€ë‹ˆì–´ë¡œ 3ë…„ê°„ ì¼í•˜ë©´ì„œ í•˜ë£¨ 1ì–µ ê±´ ì´ìƒì˜ ë¡œê·¸ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í–ˆìŠµë‹ˆë‹¤. Apache Kafkaì™€ Spark Streamingì„ ì‚¬ìš©í•´ì„œ ì‹¤ì‹œê°„ ë°ì´í„° ì²˜ë¦¬ ì‹œìŠ¤í…œì„ ë§Œë“¤ì—ˆê³ , ê¸°ì¡´ ë°°ì¹˜ ì²˜ë¦¬ ëŒ€ë¹„ ì§€ì—° ì‹œê°„ì„ 6ì‹œê°„ì—ì„œ 5ë¶„ìœ¼ë¡œ ì¤„ì˜€ìŠµë‹ˆë‹¤. ë˜í•œ Airflowë¥¼ ë„ì…í•´ì„œ 100ê°œ ì´ìƒì˜ ETL ì‘ì—…ì„ ìë™í™”í–ˆìŠµë‹ˆë‹¤.",
                "expected_occupation": "ICT",
                "expected_experience": "EXPERIENCED"
            },
            # ICT ì‹ ì…
            {
                "query": "ì»´í“¨í„°ê³µí•™ì„ ì „ê³µí•˜ë©´ì„œ Reactì™€ TypeScriptë¥¼ ì‚¬ìš©í•´ì„œ ê°œì¸ í”„ë¡œì íŠ¸ë¡œ í• ì¼ ê´€ë¦¬ ì›¹ì•±ì„ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤. Reduxë¡œ ìƒíƒœ ê´€ë¦¬ë¥¼ í•˜ê³ , Firebaseë¡œ ë°±ì—”ë“œë¥¼ êµ¬ì„±í–ˆëŠ”ë°, ì‹¤ì œë¡œ 100ëª… ì •ë„ì˜ ì‚¬ìš©ìê°€ ì‚¬ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤. ë˜í•œ GitHub Actionsë¡œ CI/CD íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•´ì„œ ìë™ ë°°í¬ í™˜ê²½ì„ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤.",
                "expected_occupation": "ICT",
                "expected_experience": "NEW"
            },
            {
                "query": "ì¡¸ì—… í”„ë¡œì íŠ¸ë¡œ ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ì£¼ì‹ ê°€ê²© ì˜ˆì¸¡ ì‹œìŠ¤í…œì„ ê°œë°œí–ˆìŠµë‹ˆë‹¤. Pythonì˜ TensorFlowì™€ Pandasë¥¼ í™œìš©í•´ì„œ LSTM ëª¨ë¸ì„ êµ¬í˜„í–ˆê³ , ë°±í…ŒìŠ¤íŒ… ê²°ê³¼ ì—° ìˆ˜ìµë¥  15%ë¥¼ ê¸°ë¡í–ˆìŠµë‹ˆë‹¤. í”„ë¡œì íŠ¸ë¥¼ ì§„í–‰í•˜ë©´ì„œ ë°ì´í„° ì „ì²˜ë¦¬ì˜ ì¤‘ìš”ì„±ê³¼ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ë°©ë²•ì„ ë°°ì› ê³ , Jupyter Notebookìœ¼ë¡œ ë¶„ì„ ê³¼ì •ì„ ë¬¸ì„œí™”í–ˆìŠµë‹ˆë‹¤.",
                "expected_occupation": "ICT",
                "expected_experience": "NEW"
            },

            # BM ê²½ë ¥
            {
                "query": "í”„ë¡œì íŠ¸ ë§¤ë‹ˆì €ë¡œ 5ë…„ê°„ ì¼í•˜ë©´ì„œ 20ëª… ê·œëª¨ì˜ ê°œë°œíŒ€ì„ ë¦¬ë“œí•˜ê³  ì—°ê°„ 50ì–µ ì› ê·œëª¨ì˜ í”„ë¡œì íŠ¸ë¥¼ ì„±ê³µì ìœ¼ë¡œ ì™„ìˆ˜í–ˆìŠµë‹ˆë‹¤. ì• ìì¼ ë°©ë²•ë¡ ì„ ë„ì…í•´ì„œ 2ì£¼ ë‹¨ìœ„ ìŠ¤í”„ë¦°íŠ¸ë¥¼ ìš´ì˜í–ˆê³ , JIRAì™€ Confluenceë¥¼ í™œìš©í•´ì„œ ì—…ë¬´ ê°€ì‹œì„±ì„ ë†’ì˜€ìŠµë‹ˆë‹¤. íŠ¹íˆ ê³ ê°ì‚¬ì™€ì˜ ì£¼ê°„ ë¯¸íŒ…ì„ í†µí•´ ìš”êµ¬ì‚¬í•­ ë³€ê²½ì„ ì¡°ê¸°ì— íŒŒì•…í•´ì„œ í”„ë¡œì íŠ¸ ì§€ì—°ìœ¨ì„ 30% ì¤„ì˜€ìŠµë‹ˆë‹¤.",
                "expected_occupation": "BM",
                "expected_experience": "EXPERIENCED"
            },
            {
                "query": "ê²½ì˜ê¸°íšíŒ€ì—ì„œ 3ë…„ê°„ ì¼í•˜ë©´ì„œ íšŒì‚¬ì˜ ì¤‘ì¥ê¸° ì „ëµ ìˆ˜ë¦½ê³¼ ì‚¬ì—…ê³„íšì„œ ì‘ì„±ì„ ë‹´ë‹¹í–ˆìŠµë‹ˆë‹¤. ê²½ìŸì‚¬ ë¶„ì„ê³¼ ì‹œì¥ ì¡°ì‚¬ë¥¼ í†µí•´ ì‹ ê·œ ì‚¬ì—… ê¸°íšŒë¥¼ ë°œêµ´í–ˆê³ , ê·¸ ê²°ê³¼ ì‹ ê·œ ì‚¬ì—… ë¶€ë¬¸ì—ì„œ ì²« í•´ ë§¤ì¶œ 30ì–µ ì›ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. ë˜í•œ BSC ê¸°ë°˜ì˜ ì„±ê³¼ê´€ë¦¬ ì²´ê³„ë¥¼ ë„ì…í•´ì„œ ë¶€ì„œë³„ KPIë¥¼ ì²´ê³„í™”í–ˆìŠµë‹ˆë‹¤.",
                "expected_occupation": "BM",
                "expected_experience": "EXPERIENCED"
            },
            # BM ì‹ ì…
            {
                "query": "ê²½ì˜í•™ì„ ì „ê³µí•˜ë©´ì„œ ì°½ì—… ë™ì•„ë¦¬ íšŒì¥ì„ 2ë…„ê°„ ë§¡ì•˜ìŠµë‹ˆë‹¤. 50ëª…ì˜ ë™ì•„ë¦¬ íšŒì›ì„ ì´ëŒë©´ì„œ ë¶„ê¸°ë³„ ì°½ì—… ê²½ì§„ëŒ€íšŒë¥¼ ê¸°íší–ˆê³ , ì´ 20ê°œ íŒ€ì´ ì°¸ê°€í•˜ëŠ” ëŒ€íšŒë¡œ ì„±ì¥ì‹œì¼°ìŠµë‹ˆë‹¤. íŠ¹íˆ ì™¸ë¶€ íˆ¬ììì™€ ë©˜í† ë¥¼ ì„­ì™¸í•˜ëŠ” ë„¤íŠ¸ì›Œí‚¹ í™œë™ì„ ì£¼ë„í–ˆê³ , ê·¸ ê²°ê³¼ 3ê°œ íŒ€ì´ ì‹¤ì œ íˆ¬ì ìœ ì¹˜ì— ì„±ê³µí–ˆìŠµë‹ˆë‹¤.",
                "expected_occupation": "BM",
                "expected_experience": "NEW"
            },
            {
                "query": "ëŒ€í•™êµì—ì„œ í•™ìƒíšŒ ì¬ì •ë¶€ì¥ì„ ë§¡ì•„ ì—°ê°„ 5ì²œë§Œ ì›ì˜ ì˜ˆì‚°ì„ ê´€ë¦¬í–ˆìŠµë‹ˆë‹¤. ì—‘ì…€ë¡œ ìˆ˜ì…/ì§€ì¶œ ê´€ë¦¬ ì‹œìŠ¤í…œì„ ë§Œë“¤ì–´ì„œ íšŒê³„ íˆ¬ëª…ì„±ì„ ë†’ì˜€ê³ , ë¶„ê¸°ë³„ ê²°ì‚° ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì„œ í•™ìƒë“¤ì—ê²Œ ê³µê°œí–ˆìŠµë‹ˆë‹¤. ì´ ê²½í—˜ì„ í†µí•´ ì˜ˆì‚° í¸ì„±ê³¼ ì§‘í–‰, ê·¸ë¦¬ê³  ì´í•´ê´€ê³„ì ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ì˜ ì¤‘ìš”ì„±ì„ ë°°ì› ìŠµë‹ˆë‹¤.",
                "expected_occupation": "BM",
                "expected_experience": "NEW"
            },

            # SM ê²½ë ¥
            {
                "query": "B2B ì˜ì—…íŒ€ì—ì„œ 5ë…„ê°„ ì¼í•˜ë©´ì„œ ëŒ€ê¸°ì—… ê³ ê°ì„ ì „ë‹´í–ˆìŠµë‹ˆë‹¤. ì—°ê°„ ì˜ì—… ëª©í‘œ 100ì–µ ì›ì„ 3ë…„ ì—°ì† ë‹¬ì„±í–ˆê³ , ì‹ ê·œ ëŒ€ê¸°ì—… ê³ ê° 5ê³³ì„ ìœ ì¹˜í–ˆìŠµë‹ˆë‹¤. íŠ¹íˆ 6ê°œì›”ê°„ì˜ í˜‘ìƒ ëì— ëŒ€í˜• ì œì¡°ì‚¬ì™€ 3ë…„ê°„ 150ì–µ ì› ê·œëª¨ì˜ ì¥ê¸° ê³„ì•½ì„ ì²´ê²°í–ˆëŠ”ë°, ì´ ê³¼ì •ì—ì„œ ê³ ê°ì˜ pain pointë¥¼ ì •í™•íˆ íŒŒì•…í•˜ê³  ë§ì¶¤í˜• ì†”ë£¨ì…˜ì„ ì œì•ˆí•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤ëŠ” ê²ƒì„ ë°°ì› ìŠµë‹ˆë‹¤.",
                "expected_occupation": "SM",
                "expected_experience": "EXPERIENCED"
            },
            {
                "query": "ë””ì§€í„¸ ë§ˆì¼€íŒ… íŒ€ì¥ìœ¼ë¡œ 3ë…„ê°„ ì¼í•˜ë©´ì„œ ì—°ê°„ 10ì–µ ì›ì˜ ë§ˆì¼€íŒ… ì˜ˆì‚°ì„ ìš´ì˜í–ˆìŠµë‹ˆë‹¤. Google Analyticsì™€ Meta Ads Managerë¥¼ í™œìš©í•´ì„œ ìº í˜ì¸ ì„±ê³¼ë¥¼ ë¶„ì„í–ˆê³ , A/B í…ŒìŠ¤íŠ¸ë¥¼ í†µí•´ ì „í™˜ìœ¨ì„ í‰ê·  40% ê°œì„ í–ˆìŠµë‹ˆë‹¤. íŠ¹íˆ ë¦¬íƒ€ê²ŒíŒ… ìº í˜ì¸ì„ ë„ì…í•´ì„œ ê³ ê° íšë“ ë¹„ìš©(CAC)ì„ 50% ì ˆê°í–ˆê³ , ì´ ì„±ê³¼ë¡œ ì‚¬ë‚´ ìš°ìˆ˜ ì‚¬ì›ìœ¼ë¡œ ì„ ì •ë˜ì—ˆìŠµë‹ˆë‹¤.",
                "expected_occupation": "SM",
                "expected_experience": "EXPERIENCED"
            },
            # SM ì‹ ì…
            {
                "query": "ëŒ€í•™êµ ë§ˆì¼€íŒ… ê³µëª¨ì „ì—ì„œ ëŒ€ìƒì„ ìˆ˜ìƒí•œ ê²½í—˜ì´ ìˆìŠµë‹ˆë‹¤. MZì„¸ëŒ€ë¥¼ íƒ€ê²Ÿìœ¼ë¡œ í•œ SNS ë°”ì´ëŸ´ ë§ˆì¼€íŒ… ì „ëµì„ ì œì•ˆí–ˆëŠ”ë°, ì¸ìŠ¤íƒ€ê·¸ë¨ê³¼ í‹±í†¡ì„ í™œìš©í•œ ì±Œë¦°ì§€ ìº í˜ì¸ ê¸°íšìœ¼ë¡œ ì˜ˆìƒ ë„ë‹¬ë¥  200ë§Œ ë·°ë¥¼ ì‚°ì¶œí–ˆìŠµë‹ˆë‹¤. ì‹¤ì œë¡œ ì‹œë®¬ë ˆì´ì…˜ì„ í†µí•´ ë¹„ìš© ëŒ€ë¹„ íš¨ê³¼ë¥¼ ë¶„ì„í–ˆê³ , ì´ ê²½í—˜ì„ í†µí•´ ë°ì´í„° ê¸°ë°˜ ë§ˆì¼€íŒ…ì˜ ì¤‘ìš”ì„±ì„ ê¹¨ë‹¬ì•˜ìŠµë‹ˆë‹¤.",
                "expected_occupation": "SM",
                "expected_experience": "NEW"
            },
            {
                "query": "ìœ í†µí•™ê³¼ë¥¼ ì „ê³µí•˜ë©´ì„œ êµë‚´ ì¹´í˜ì—ì„œ 2ë…„ê°„ ë§¤ë‹ˆì €ë¡œ ì¼í–ˆìŠµë‹ˆë‹¤. ì›” ë§¤ì¶œ ê´€ë¦¬ì™€ ì¬ê³  ê´€ë¦¬, ê·¸ë¦¬ê³  5ëª…ì˜ ì•„ë¥´ë°”ì´íŠ¸ìƒ ìŠ¤ì¼€ì¤„ ê´€ë¦¬ë¥¼ ë‹´ë‹¹í–ˆìŠµë‹ˆë‹¤. SNS ë§ˆì¼€íŒ…ì„ ë„ì…í•´ì„œ ì¸ìŠ¤íƒ€ê·¸ë¨ íŒ”ë¡œì›Œë¥¼ 500ëª…ì—ì„œ 3000ëª…ìœ¼ë¡œ ëŠ˜ë ¸ê³ , ì´ë²¤íŠ¸ í”„ë¡œëª¨ì…˜ì„ í†µí•´ ì›” ë§¤ì¶œì„ 20% ì„±ì¥ì‹œì¼°ìŠµë‹ˆë‹¤.",
                "expected_occupation": "SM",
                "expected_experience": "NEW"
            },

            # RND ê²½ë ¥
            {
                "query": "AI ì—°êµ¬ì†Œì—ì„œ 5ë…„ê°„ ìì—°ì–´ ì²˜ë¦¬ ì—°êµ¬ë¥¼ í–ˆìŠµë‹ˆë‹¤. BERTì™€ GPT ê¸°ë°˜ì˜ í•œêµ­ì–´ ê°ì„± ë¶„ì„ ëª¨ë¸ì„ ê°œë°œí•´ì„œ SCIê¸‰ ì €ë„ì— ë…¼ë¬¸ 3í¸ì„ ê²Œì¬í–ˆê³ , íŠ¹í—ˆ 2ê±´ì„ ì¶œì›í–ˆìŠµë‹ˆë‹¤. ë˜í•œ ì—°êµ¬ ê²°ê³¼ë¥¼ ìƒìš©í™”í•´ì„œ ê³ ê° ë¦¬ë·° ë¶„ì„ ì„œë¹„ìŠ¤ë¥¼ ì¶œì‹œí–ˆëŠ”ë°, í˜„ì¬ ì›”ê°„ 100ë§Œ ê±´ ì´ìƒì˜ ë¦¬ë·°ë¥¼ ì²˜ë¦¬í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì—°êµ¬ì™€ ì‹¤ë¬´ì˜ ê°„ê·¹ì„ ì¤„ì´ëŠ” ê²ƒì´ ì œ ê°•ì ì…ë‹ˆë‹¤.",
                "expected_occupation": "RND",
                "expected_experience": "EXPERIENCED"
            },
            # RND ì‹ ì…
            {
                "query": "ì„ì‚¬ ê³¼ì •ì—ì„œ ì»´í“¨í„° ë¹„ì „ ì—°êµ¬ë¥¼ í–ˆìŠµë‹ˆë‹¤. ì˜ë£Œ ì˜ìƒì—ì„œ ì•” ì¡°ê¸° ì§„ë‹¨ì„ ìœ„í•œ ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ ê°œë°œí–ˆëŠ”ë°, CNNê³¼ Attention ë©”ì»¤ë‹ˆì¦˜ì„ ê²°í•©í•´ì„œ ê¸°ì¡´ ëª¨ë¸ ëŒ€ë¹„ ì •í™•ë„ë¥¼ 5% í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤. ì´ ì—°êµ¬ë¡œ êµ­ë‚´ í•™íšŒì—ì„œ ìš°ìˆ˜ ë…¼ë¬¸ìƒì„ ë°›ì•˜ê³ , í˜„ì¬ SCI ì €ë„ íˆ¬ê³ ë¥¼ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤. PyTorchì™€ OpenCVë¥¼ ëŠ¥ìˆ™í•˜ê²Œ ë‹¤ë£° ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                "expected_occupation": "RND",
                "expected_experience": "NEW"
            },

            # ì¼ë°˜/ìƒí™© ì§ˆë¬¸
            {
                "query": "í”„ë¡œì íŠ¸ ì§„í–‰ ì¤‘ ê¸°ìˆ ì ìœ¼ë¡œ í•´ê²°ì´ ì–´ë ¤ìš´ ë¬¸ì œì— ì§ë©´í•œ ì ì´ ìˆìŠµë‹ˆë‹¤. ë ˆê±°ì‹œ ì‹œìŠ¤í…œê³¼ ì‹ ê·œ ì‹œìŠ¤í…œ ê°„ì˜ ë°ì´í„° ì—°ë™ ë¬¸ì œì˜€ëŠ”ë°, ë¨¼ì € ë¬¸ì œì˜ ê·¼ë³¸ ì›ì¸ì„ íŒŒì•…í•˜ê¸° ìœ„í•´ ë¡œê·¸ ë¶„ì„ê³¼ ë””ë²„ê¹…ì„ í–ˆìŠµë‹ˆë‹¤. ê·¸ ê²°ê³¼ ë°ì´í„° í¬ë§· ë¶ˆì¼ì¹˜ê°€ ì›ì¸ì„ì„ ë°œê²¬í–ˆê³ , ì¤‘ê°„ì— ë³€í™˜ ë ˆì´ì–´ë¥¼ ì¶”ê°€í•´ì„œ í•´ê²°í–ˆìŠµë‹ˆë‹¤. ì´ ê²½í—˜ì„ í†µí•´ ë¬¸ì œë¥¼ ì²´ê³„ì ìœ¼ë¡œ ë¶„ì„í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤ëŠ” ê²ƒì„ ë°°ì› ìŠµë‹ˆë‹¤.",
                "expected_occupation": "ICT",
                "expected_experience": "EXPERIENCED"
            },
            {
                "query": "íŒ€ì› ê°„ ì˜ê²¬ ì¶©ëŒì´ ìˆì—ˆë˜ ê²½í—˜ì´ ìˆìŠµë‹ˆë‹¤. í”„ë¡œì íŠ¸ ë°©í–¥ì— ëŒ€í•´ ë‘ íŒ€ì›ì´ ë‹¤ë¥¸ ì˜ê²¬ì„ ê°€ì§€ê³  ìˆì—ˆëŠ”ë°, ì €ëŠ” ë¨¼ì € ê°ìì˜ ì˜ê²¬ì„ ì¶©ë¶„íˆ ê²½ì²­í–ˆìŠµë‹ˆë‹¤. ê·¸ë¦¬ê³  ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê° ë°©ì•ˆì˜ ì¥ë‹¨ì ì„ ë¶„ì„í•˜ëŠ” íšŒì˜ë¥¼ ì£¼ì„ í–ˆê³ , ìµœì¢…ì ìœ¼ë¡œ ë‘ ì˜ê²¬ì˜ ì¥ì ì„ ê²°í•©í•œ ì ˆì¶©ì•ˆì„ ë„ì¶œí–ˆìŠµë‹ˆë‹¤. ê²°ê³¼ì ìœ¼ë¡œ í”„ë¡œì íŠ¸ëŠ” ì˜ˆì •ë³´ë‹¤ ë¹¨ë¦¬ ì™„ë£Œë˜ì—ˆê³ , íŒ€ ë¶„ìœ„ê¸°ë„ ì¢‹ì•„ì¡ŒìŠµë‹ˆë‹¤.",
                "expected_occupation": "BM",
                "expected_experience": "EXPERIENCED"
            },
            {
                "query": "ê³ ê° í´ë ˆì„ì„ ì²˜ë¦¬í•œ ê²½í—˜ì´ ìˆìŠµë‹ˆë‹¤. ë°°ì†¡ ì§€ì—°ìœ¼ë¡œ ì¸í•´ ë§¤ìš° í™”ê°€ ë‚œ ê³ ê°ì´ì—ˆëŠ”ë°, ë¨¼ì € ì¶©ë¶„íˆ ì‚¬ê³¼í•˜ê³  ê³ ê°ì˜ ë¶ˆë§Œì„ ê²½ì²­í–ˆìŠµë‹ˆë‹¤. ê·¸ í›„ ë°°ì†¡ ì§€ì—° ì›ì¸ì„ í™•ì¸í•˜ê³  ì˜ˆìƒ ë„ì°© ì‹œê°„ì„ ì•ˆë‚´í–ˆìœ¼ë©°, ë³´ìƒìœ¼ë¡œ í• ì¸ ì¿ í°ì„ ì œê³µí–ˆìŠµë‹ˆë‹¤. ê³ ê°ì€ ì²˜ìŒì—ëŠ” í™”ê°€ ë‚˜ì…¨ì§€ë§Œ, ì„±ì˜ ìˆëŠ” ëŒ€ì‘ì— ì˜¤íˆë ¤ ê°ì‚¬í•˜ë‹¤ë©° ë‹¤ìŒì—ë„ ì´ìš©í•˜ê² ë‹¤ê³  ë§ì”€í•´ì£¼ì…¨ìŠµë‹ˆë‹¤.",
                "expected_occupation": "SM",
                "expected_experience": "EXPERIENCED"
            },
            {
                "query": "5ë…„ í›„ì—ëŠ” í•´ë‹¹ ë¶„ì•¼ì˜ ì „ë¬¸ê°€ë¡œ ì¸ì •ë°›ê³  ì‹¶ìŠµë‹ˆë‹¤. ë¨¼ì € ì…ì‚¬ í›„ 2ë…„ê°„ì€ ì‹¤ë¬´ ì—­ëŸ‰ì„ ìŒ“ê³  ê´€ë ¨ ìê²©ì¦ì„ ì·¨ë“í•  ê³„íšì…ë‹ˆë‹¤. ê·¸ ì´í›„ì—ëŠ” í›„ë°°ë“¤ì„ ë©˜í† ë§í•˜ë©´ì„œ íŒ€ì˜ ìƒì‚°ì„±ì„ ë†’ì´ëŠ” ë° ê¸°ì—¬í•˜ê³  ì‹¶ê³ , 5ë…„ í›„ì—ëŠ” íŒ€ì¥ê¸‰ ë¦¬ë”ë¡œì„œ í”„ë¡œì íŠ¸ë¥¼ ì£¼ë„í•˜ê³  ì‹¶ìŠµë‹ˆë‹¤. ë˜í•œ ì—…ê³„ ì»¨í¼ëŸ°ìŠ¤ì—ì„œ ë°œí‘œí•  ìˆ˜ ìˆëŠ” ìˆ˜ì¤€ì˜ ì „ë¬¸ì„±ì„ ê°–ì¶”ê³  ì‹¶ìŠµë‹ˆë‹¤.",
                "expected_occupation": "ICT",
                "expected_experience": "NEW"
            },
            {
                "query": "íšŒì‚¬ë¥¼ ì„ íƒí•  ë•Œ ê°€ì¥ ì¤‘ìš”í•˜ê²Œ ìƒê°í•˜ëŠ” ê²ƒì€ ì„±ì¥ ê°€ëŠ¥ì„±ì…ë‹ˆë‹¤. ì´ íšŒì‚¬ëŠ” ì—…ê³„ì—ì„œ í˜ì‹ ì ì¸ ê¸°ìˆ ë¡œ ë¹ ë¥´ê²Œ ì„±ì¥í•˜ê³  ìˆê³ , ì§ì› êµìœ¡ì—ë„ ë§ì€ íˆ¬ìë¥¼ í•œë‹¤ê³  ë“¤ì—ˆìŠµë‹ˆë‹¤. íŠ¹íˆ ì‚¬ë‚´ ìŠ¤í„°ë””ì™€ ì»¨í¼ëŸ°ìŠ¤ ì°¸ê°€ ì§€ì› ì œë„ê°€ ì¸ìƒì ì´ì—ˆìŠµë‹ˆë‹¤. ì €ëŠ” ì´ëŸ° í™˜ê²½ì—ì„œ ë¹ ë¥´ê²Œ ì„±ì¥í•´ì„œ íšŒì‚¬ì˜ ì„±ì¥ì—ë„ ê¸°ì—¬í•˜ê³  ì‹¶ìŠµë‹ˆë‹¤.",
                "expected_occupation": "BM",
                "expected_experience": "NEW"
            },
            {
                "query": "ì €ì˜ ë‹¨ì ì€ ì™„ë²½ì£¼ì˜ì ì¸ ì„±í–¥ì…ë‹ˆë‹¤. ì²˜ìŒì—ëŠ” ëª¨ë“  ê²ƒì„ ì™„ë²½í•˜ê²Œ í•˜ë ¤ë‹¤ ë³´ë‹ˆ ì—…ë¬´ ì²˜ë¦¬ ì†ë„ê°€ ëŠë ¸ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì´ ë¬¸ì œë¥¼ ì¸ì‹í•˜ê³ , ìš°ì„ ìˆœìœ„ë¥¼ ì •í•´ì„œ ì¤‘ìš”í•œ ê²ƒë¶€í„° ì²˜ë¦¬í•˜ëŠ” ë°©ë²•ì„ ë°°ì› ìŠµë‹ˆë‹¤. ë˜í•œ 80%ì˜ ì™„ì„±ë„ë¡œ ë¨¼ì € í”¼ë“œë°±ì„ ë°›ê³  ê°œì„ í•˜ëŠ” ê²ƒì´ ë” íš¨ìœ¨ì ì´ë¼ëŠ” ê²ƒì„ ê¹¨ë‹¬ì•˜ê³ , ì§€ê¸ˆì€ ë°ë“œë¼ì¸ì„ ì¤€ìˆ˜í•˜ë©´ì„œë„ í’ˆì§ˆì„ ìœ ì§€í•˜ëŠ” ê· í˜•ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.",
                "expected_occupation": "ICT",
                "expected_experience": "EXPERIENCED"
            },
        ]

        # ìƒ˜í”Œë§
        if n_samples < len(test_cases):
            return random.sample(test_cases, n_samples)
        return test_cases

    def evaluate_retrieval(
        self,
        query: str,
        expected_occupation: str,
        expected_experience: str,
        k: int = 3
    ) -> RetrievalMetrics:
        """
        ê²€ìƒ‰ í’ˆì§ˆ í‰ê°€

        Args:
            query: í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
            expected_occupation: ê¸°ëŒ€í•˜ëŠ” ì§ì—…êµ°
            expected_experience: ê¸°ëŒ€í•˜ëŠ” ê²½ë ¥
            k: ê²€ìƒ‰í•  ë¬¸ì„œ ìˆ˜

        Returns:
            RetrievalMetrics: ê²€ìƒ‰ í’ˆì§ˆ ì§€í‘œ
        """
        start_time = time.time()

        # ê²€ìƒ‰ ìˆ˜í–‰
        results = self.rag.retrieve(query, k=k)

        retrieval_time = (time.time() - start_time) * 1000  # ms

        # ì¼ì¹˜ìœ¨ ê³„ì‚°
        occupation_matches = sum(
            1 for r in results
            if r.get("occupation", "").upper() == expected_occupation.upper()
        )
        experience_matches = sum(
            1 for r in results
            if r.get("experience", "").upper() == expected_experience.upper()
        )

        occupation_match_rate = occupation_matches / len(results) if results else 0
        experience_match_rate = experience_matches / len(results) if results else 0

        return RetrievalMetrics(
            query=query,
            expected_occupation=expected_occupation,
            expected_experience=expected_experience,
            retrieved_docs=results,
            occupation_match_rate=occupation_match_rate,
            experience_match_rate=experience_match_rate,
            avg_similarity_score=0.0,  # FAISSëŠ” ì§ì ‘ ìŠ¤ì½”ì–´ ë°˜í™˜ ì•ˆí•¨
            retrieval_time_ms=retrieval_time
        )

    def evaluate_generation(
        self,
        query: str,
        occupation: Optional[str] = None,
        experience: Optional[str] = None
    ) -> GenerationMetrics:
        """
        ìƒì„± í’ˆì§ˆ í‰ê°€

        Args:
            query: í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
            occupation: ì§ì—…êµ° í•„í„°
            experience: ê²½ë ¥ í•„í„°

        Returns:
            GenerationMetrics: ìƒì„± í’ˆì§ˆ ì§€í‘œ
        """
        start_time = time.time()

        # ì‘ë‹µ ìƒì„±
        response = self.rag.generate(query, occupation, experience)

        generation_time = (time.time() - start_time) * 1000  # ms

        # í’ˆì§ˆ ì²´í¬
        is_korean = any('\uac00' <= c <= '\ud7a3' for c in response)  # í•œê¸€ í¬í•¨
        is_question = response.strip().endswith("?") or "?" in response
        has_context = any(
            keyword in response.lower()
            for keyword in ["ë‹µë³€", "ê²½í—˜", "ë§ì”€", "ì§ˆë¬¸", "ì–´ë–»ê²Œ", "ì™œ"]
        )

        return GenerationMetrics(
            query=query,
            response=response,
            response_length=len(response),
            generation_time_ms=generation_time,
            is_korean=is_korean,
            is_question_format=is_question,
            has_context_reference=has_context
        )

    def run_evaluation(
        self,
        n_samples: int = 10,
        include_generation: bool = True,
        save_results: bool = True,
        output_dir: Optional[Path] = None
    ) -> EvaluationResult:
        """
        ì „ì²´ í‰ê°€ ì‹¤í–‰

        Args:
            n_samples: í…ŒìŠ¤íŠ¸í•  ìƒ˜í”Œ ìˆ˜
            include_generation: ìƒì„± í‰ê°€ í¬í•¨ ì—¬ë¶€
            save_results: ê²°ê³¼ ì €ì¥ ì—¬ë¶€
            output_dir: ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬

        Returns:
            EvaluationResult: ì „ì²´ í‰ê°€ ê²°ê³¼
        """
        print(f"\n{'='*60}")
        print(f"RAG ì‹¤íš¨ì„± í‰ê°€ ì‹œì‘ (ìƒ˜í”Œ ìˆ˜: {n_samples})")
        print(f"{'='*60}\n")

        test_queries = self.create_test_queries(n_samples)

        retrieval_results = []
        generation_results = []

        # ê²€ìƒ‰ í‰ê°€
        print("[1/2] ê²€ìƒ‰ í’ˆì§ˆ í‰ê°€ ì¤‘...")
        for i, tc in enumerate(test_queries):
            print(f"  [{i+1}/{len(test_queries)}] {tc['query'][:40]}...")

            metrics = self.evaluate_retrieval(
                tc["query"],
                tc["expected_occupation"],
                tc["expected_experience"]
            )
            retrieval_results.append(asdict(metrics))

        # ìƒì„± í‰ê°€
        if include_generation:
            print("\n[2/2] ìƒì„± í’ˆì§ˆ í‰ê°€ ì¤‘...")
            for i, tc in enumerate(test_queries):
                print(f"  [{i+1}/{len(test_queries)}] {tc['query'][:40]}...")

                metrics = self.evaluate_generation(tc["query"])
                generation_results.append(asdict(metrics))

        # ì§‘ê³„
        avg_occupation_match = sum(
            r["occupation_match_rate"] for r in retrieval_results
        ) / len(retrieval_results)

        avg_experience_match = sum(
            r["experience_match_rate"] for r in retrieval_results
        ) / len(retrieval_results)

        avg_retrieval_time = sum(
            r["retrieval_time_ms"] for r in retrieval_results
        ) / len(retrieval_results)

        if generation_results:
            avg_response_length = sum(
                r["response_length"] for r in generation_results
            ) / len(generation_results)

            avg_generation_time = sum(
                r["generation_time_ms"] for r in generation_results
            ) / len(generation_results)

            korean_rate = sum(
                1 for r in generation_results if r["is_korean"]
            ) / len(generation_results)

            question_format_rate = sum(
                1 for r in generation_results if r["is_question_format"]
            ) / len(generation_results)
        else:
            avg_response_length = 0
            avg_generation_time = 0
            korean_rate = 0
            question_format_rate = 0

        result = EvaluationResult(
            timestamp=datetime.now().isoformat(),
            total_queries=len(test_queries),
            avg_occupation_match=avg_occupation_match,
            avg_experience_match=avg_experience_match,
            avg_retrieval_time_ms=avg_retrieval_time,
            avg_response_length=avg_response_length,
            avg_generation_time_ms=avg_generation_time,
            korean_rate=korean_rate,
            question_format_rate=question_format_rate,
            retrieval_results=retrieval_results,
            generation_results=generation_results
        )

        # ê²°ê³¼ ì¶œë ¥
        self._print_summary(result)

        # ê²°ê³¼ ì €ì¥
        if save_results:
            self._save_results(result, output_dir)

        return result

    def _print_summary(self, result: EvaluationResult):
        """í‰ê°€ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print(f"\n{'='*60}")
        print("ğŸ“Š RAG ì‹¤íš¨ì„± í‰ê°€ ê²°ê³¼")
        print(f"{'='*60}")

        print(f"\nğŸ“‹ ê¸°ë³¸ ì •ë³´:")
        print(f"  - í‰ê°€ ì‹œê°„: {result.timestamp}")
        print(f"  - í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ìˆ˜: {result.total_queries}")

        print(f"\nğŸ” ê²€ìƒ‰ í’ˆì§ˆ (Retrieval Quality):")
        print(f"  - ì§ì—…êµ° ì¼ì¹˜ìœ¨: {result.avg_occupation_match*100:.1f}%")
        print(f"  - ê²½ë ¥ ì¼ì¹˜ìœ¨: {result.avg_experience_match*100:.1f}%")
        print(f"  - í‰ê·  ê²€ìƒ‰ ì‹œê°„: {result.avg_retrieval_time_ms:.1f}ms")

        if result.generation_results:
            print(f"\nâœï¸ ìƒì„± í’ˆì§ˆ (Generation Quality):")
            print(f"  - í‰ê·  ì‘ë‹µ ê¸¸ì´: {result.avg_response_length:.0f}ì")
            print(f"  - í‰ê·  ìƒì„± ì‹œê°„: {result.avg_generation_time_ms:.1f}ms")
            print(f"  - í•œê¸€ ì‘ë‹µ ë¹„ìœ¨: {result.korean_rate*100:.1f}%")
            print(f"  - ì§ˆë¬¸ í˜•ì‹ ë¹„ìœ¨: {result.question_format_rate*100:.1f}%")

        # ì‹¤íš¨ì„± íŒì •
        print(f"\n{'='*60}")
        print("ğŸ“ˆ RAG ì‹¤íš¨ì„± íŒì •:")

        retrieval_score = (result.avg_occupation_match + result.avg_experience_match) / 2

        if retrieval_score >= 0.5:
            print("  âœ… ê²€ìƒ‰ í’ˆì§ˆ: ì–‘í˜¸ (ê´€ë ¨ ë¬¸ì„œë¥¼ ì˜ ì°¾ê³  ìˆìŒ)")
        elif retrieval_score >= 0.3:
            print("  âš ï¸ ê²€ìƒ‰ í’ˆì§ˆ: ë³´í†µ (ì¼ë¶€ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ìŒ)")
        else:
            print("  âŒ ê²€ìƒ‰ í’ˆì§ˆ: ê°œì„  í•„ìš” (ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ì´ ë¶€ì¡±í•¨)")

        if result.korean_rate >= 0.9 and result.question_format_rate >= 0.5:
            print("  âœ… ìƒì„± í’ˆì§ˆ: ì–‘í˜¸ (ë©´ì ‘ê´€ ì—­í•  ìˆ˜í–‰ ì¤‘)")
        elif result.korean_rate >= 0.7:
            print("  âš ï¸ ìƒì„± í’ˆì§ˆ: ë³´í†µ (ì¼ë¶€ ê°œì„  í•„ìš”)")
        else:
            print("  âŒ ìƒì„± í’ˆì§ˆ: ê°œì„  í•„ìš”")

        print(f"{'='*60}\n")

    def _save_results(
        self,
        result: EvaluationResult,
        output_dir: Optional[Path] = None
    ):
        """í‰ê°€ ê²°ê³¼ ì €ì¥"""
        if output_dir is None:
            output_dir = Path(__file__).parent / "evaluation_results"

        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = output_dir / f"evaluation_{timestamp}.json"

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(asdict(result), f, ensure_ascii=False, indent=2)

        print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥ë¨: {output_file}")

    def _is_question_format(self, response: str) -> bool:
        """
        í•œêµ­ì–´ ì§ˆë¬¸ í˜•ì‹ ì²´í¬

        ì§ì ‘ ì§ˆë¬¸(?)ë¿ ì•„ë‹ˆë¼ í•œêµ­ì–´ ê°„ì ‘ ì§ˆë¬¸ í˜•ì‹ë„ ì¸ì‹
        ì˜ˆ: "ê¶ê¸ˆí•©ë‹ˆë‹¤", "ì•Œê³  ì‹¶ìŠµë‹ˆë‹¤", "ë§ì”€í•´ì£¼ì„¸ìš”" ë“±
        """
        # ì§ì ‘ ì§ˆë¬¸
        if "?" in response:
            return True

        # ê°„ì ‘ ì§ˆë¬¸ í˜•ì‹ (í•œêµ­ì–´ íŠ¹ìœ ì˜ í‘œí˜„)
        indirect_patterns = [
            "ê¶ê¸ˆí•©ë‹ˆë‹¤", "ê¶ê¸ˆí•´ìš”", "ê¶ê¸ˆí•œë°ìš”",
            "ì•Œê³  ì‹¶ìŠµë‹ˆë‹¤", "ì•Œê³  ì‹¶ì–´ìš”", "ì•Œê³  ì‹¶ì€ë°ìš”",
            "ë§ì”€í•´ì£¼ì„¸ìš”", "ë§ì”€í•´ì£¼ì‹œê² ", "ë§í•´ì£¼ì„¸ìš”",
            "ì„¤ëª…í•´ì£¼ì„¸ìš”", "ì„¤ëª…í•´ì£¼ì‹œê² ",
            "ì•Œë ¤ì£¼ì„¸ìš”", "ì•Œë ¤ì£¼ì‹œê² ",
            "í•´ì£¼ì‹œê² ì–´ìš”", "í•´ì£¼ì‹¤ ìˆ˜",
            "ì‹¶ìŠµë‹ˆë‹¤", "ì‹¶ì–´ìš”",
            "ë˜ë‚˜ìš”", "ë ê¹Œìš”", "ë˜ê² ì–´ìš”",
            "ìˆë‚˜ìš”", "ìˆì„ê¹Œìš”", "ìˆê² ì–´ìš”",
            "í–ˆë‚˜ìš”", "í•˜ì…¨ë‚˜ìš”", "í•˜ì…¨ì–´ìš”",
            "ì¸ê°€ìš”", "ì¼ê¹Œìš”",
            "ê±´ê°€ìš”", "ê±¸ê¹Œìš”",
            "ë‚˜ìš”", "ê¹Œìš”", "ã„¹ê¹Œìš”"
        ]
        return any(p in response for p in indirect_patterns)

    def _extract_tech_keywords(self, text: str) -> List[str]:
        """
        í…ìŠ¤íŠ¸ì—ì„œ ê¸°ìˆ /ì „ë¬¸ ìš©ì–´ ì¶”ì¶œ

        RAGê°€ ê²€ìƒ‰í•œ ë¬¸ì„œì˜ ì „ë¬¸ ìš©ì–´ë¥¼ ì‘ë‹µì— ë°˜ì˜í–ˆëŠ”ì§€ í™•ì¸í•˜ê¸° ìœ„í•¨
        """
        tech_terms = [
            # ICT ê¸°ìˆ 
            "MSA", "ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤", "API", "Gateway", "ê²Œì´íŠ¸ì›¨ì´",
            "Kubernetes", "ì¿ ë²„ë„¤í‹°ìŠ¤", "Docker", "ë„ì»¤", "ì»¨í…Œì´ë„ˆ",
            "Spring", "ìŠ¤í”„ë§", "Java", "ìë°”", "Python", "íŒŒì´ì¬",
            "React", "ë¦¬ì•¡íŠ¸", "TypeScript", "íƒ€ì…ìŠ¤í¬ë¦½íŠ¸", "JavaScript",
            "Kafka", "ì¹´í”„ì¹´", "Spark", "ìŠ¤íŒŒí¬", "Hadoop", "í•˜ë‘¡",
            "TensorFlow", "í…ì„œí”Œë¡œìš°", "PyTorch", "íŒŒì´í† ì¹˜",
            "LSTM", "CNN", "RNN", "Transformer", "íŠ¸ëœìŠ¤í¬ë¨¸",
            "ë¨¸ì‹ ëŸ¬ë‹", "ë”¥ëŸ¬ë‹", "AI", "ì¸ê³µì§€ëŠ¥",
            "CI/CD", "ë°°í¬", "íŒŒì´í”„ë¼ì¸", "Jenkins", "ì  í‚¨ìŠ¤",
            "AWS", "Azure", "GCP", "í´ë¼ìš°ë“œ",
            "Redis", "ë ˆë””ìŠ¤", "MongoDB", "ëª½ê³ DB", "PostgreSQL",
            "REST", "GraphQL", "gRPC",

            # BM ìš©ì–´
            "ì• ìì¼", "ìŠ¤í”„ë¦°íŠ¸", "ìŠ¤í¬ëŸ¼", "ì¹¸ë°˜",
            "KPI", "OKR", "BSC", "ROI", "BEP",
            "í”„ë¡œì íŠ¸ ë§¤ë‹ˆì €", "PM", "PO", "ë¦¬ë“œ",
            "ì´í•´ê´€ê³„ì", "ìŠ¤í…Œì´í¬í™€ë”", "ìš”êµ¬ì‚¬í•­",

            # SM ìš©ì–´
            "A/B í…ŒìŠ¤íŠ¸", "ì „í™˜ìœ¨", "CVR", "CTR",
            "CAC", "LTV", "ROAS", "CPC", "CPM",
            "ë¦¬íƒ€ê²ŒíŒ…", "í¼ë„", "ì„¸ê·¸ë¨¼íŠ¸",
            "í¼í¬ë¨¼ìŠ¤", "ìº í˜ì¸", "íƒ€ê²ŸíŒ…",
            "SEO", "SEM", "SNS", "ë°”ì´ëŸ´",

            # RND ìš©ì–´
            "ë…¼ë¬¸", "íŠ¹í—ˆ", "ì—°êµ¬", "ì‹¤í—˜", "ëª¨ë¸",
            "ê°€ì„¤", "ê²€ì¦", "ë¶„ì„", "ë°ì´í„°ì…‹",
            "ì •í™•ë„", "ì •ë°€ë„", "ì¬í˜„ìœ¨", "F1",
            "í•˜ì´í¼íŒŒë¼ë¯¸í„°", "íŒŒì¸íŠœë‹", "ì „ì´í•™ìŠµ"
        ]
        # ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´ ë§¤ì¹­
        return [t for t in tech_terms if t.lower() in text.lower()]

    def _calculate_specificity_score(self, response: str, query: str) -> float:
        """
        ì‘ë‹µì˜ êµ¬ì²´ì„± ì ìˆ˜ ê³„ì‚° (0~1)

        ê°œì„ ëœ ê¸°ì¤€:
        - ì§ˆë¬¸ í˜•ì‹ ì—¬ë¶€ (+0.2) - í•œêµ­ì–´ ê°„ì ‘ ì§ˆë¬¸ í¬í•¨
        - ì¿¼ë¦¬ì˜ ê¸°ìˆ /ì „ë¬¸ ìš©ì–´ ë°˜ì˜ (+0.3) - RAG íŠ¹í™”
        - êµ¬ì²´ì  í›„ì† ì§ˆë¬¸ íŒ¨í„´ (+0.3) - ì‹¬ì¸µ ì§ˆë¬¸ ìœ ë„
        - ì ì ˆí•œ ì‘ë‹µ ê¸¸ì´ (+0.2)
        """
        score = 0.0

        # 1. ì§ˆë¬¸ í˜•ì‹ (ê°œì„ : í•œêµ­ì–´ ê°„ì ‘ ì§ˆë¬¸ í¬í•¨)
        if self._is_question_format(response):
            score += 0.2

        # 2. ì¿¼ë¦¬ì˜ ê¸°ìˆ /ì „ë¬¸ ìš©ì–´ ë°˜ì˜ (RAGê°€ ê²€ìƒ‰í•œ ë¬¸ì„œ ê¸°ë°˜ ì‘ë‹µì¸ì§€)
        # ì¿¼ë¦¬ì—ì„œ ê¸°ìˆ /ì „ë¬¸ ìš©ì–´ ì¶”ì¶œ
        query_tech_keywords = self._extract_tech_keywords(query)
        # ì‘ë‹µì— ì¿¼ë¦¬ì˜ ê¸°ìˆ  ìš©ì–´ê°€ ë°˜ì˜ë˜ì—ˆëŠ”ì§€ í™•ì¸
        response_tech_keywords = [k for k in query_tech_keywords if k.lower() in response.lower()]

        if query_tech_keywords and len(response_tech_keywords) >= 1:
            score += 0.3

        # 3. êµ¬ì²´ì  í›„ì† ì§ˆë¬¸ íŒ¨í„´ (ë‹¨ê³„ì  ì ìˆ˜)
        specific_patterns = [
            # êµ¬ì²´ì  ì‚¬ë¡€/ê²½í—˜ ìš”ì²­
            "ì˜ˆë¥¼ ë“¤ì–´", "êµ¬ì²´ì ìœ¼ë¡œ", "ì‹¤ì œë¡œ", "ì‚¬ë¡€",
            # ì‹¬ì¸µ ë¶„ì„ ìš”ì²­
            "ì–´ë–¤ ë°©ì‹", "ì–´ë–¤ ì „ëµ", "ì–´ë–¤ ê¸°ìˆ ", "ì–´ë–¤ ë„ì „", "ì–´ë–¤ ì–´ë ¤ì›€",
            # ê²°ê³¼/ì„±ê³¼ ê²€ì¦
            "ê²°ê³¼ëŠ”", "ì„±ê³¼ëŠ”", "íš¨ê³¼ëŠ”", "ê°œì„ ", "í–¥ìƒ",
            # ë¬¸ì œ í•´ê²° ê³¼ì •
            "ì–´ë–»ê²Œ í•´ê²°", "ì–´ë–»ê²Œ ê°œì„ ", "ì–´ë–»ê²Œ ê·¹ë³µ", "ì–´ë–»ê²Œ ëŒ€ì²˜",
            # ì˜ì‚¬ê²°ì •/íŒë‹¨
            "ì™œ", "ì´ìœ ", "ë°°ê²½", "íŒë‹¨", "ì„ íƒ",
            # í˜‘ì—…/ì»¤ë®¤ë‹ˆì¼€ì´ì…˜
            "íŒ€ì›", "í˜‘ì—…", "ê°ˆë“±", "ì¡°ìœ¨", "ì†Œí†µ"
        ]
        specific_count = sum(1 for p in specific_patterns if p in response)

        if specific_count >= 2:
            score += 0.3
        elif specific_count >= 1:
            score += 0.15

        # 4. ì ì ˆí•œ ì‘ë‹µ ê¸¸ì´ (ë„ˆë¬´ ì§§ì§€ë„ ê¸¸ì§€ë„ ì•Šì€)
        if 30 <= len(response) <= 200:
            score += 0.2

        return min(score, 1.0)

    def evaluate_comparison(self, query: str) -> ComparisonMetrics:
        """
        ë™ì¼ ì¿¼ë¦¬ì— ëŒ€í•´ RAG vs non-RAG ë¹„êµ í‰ê°€

        Args:
            query: í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬

        Returns:
            ComparisonMetrics: ë¹„êµ ê²°ê³¼
        """
        # 1. RAG ì‘ë‹µ ìƒì„±
        start_time = time.time()
        rag_response = self.rag.generate(query)
        rag_time = (time.time() - start_time) * 1000

        # 2. non-RAG ì‘ë‹µ ìƒì„±
        no_rag_chain = create_no_rag_chain(
            model=self.rag.model,
            temperature=self.rag.temperature
        )
        start_time = time.time()
        no_rag_response = no_rag_chain.invoke(query)
        no_rag_time = (time.time() - start_time) * 1000

        # 3. í’ˆì§ˆ ë¶„ì„ (ê°œì„ : í•œêµ­ì–´ ê°„ì ‘ ì§ˆë¬¸ í¬í•¨)
        rag_is_question = self._is_question_format(rag_response)
        no_rag_is_question = self._is_question_format(no_rag_response)

        context_keywords = ["ê²½í—˜", "í”„ë¡œì íŠ¸", "ê¸°ìˆ ", "ì–´ë–»ê²Œ", "ì™œ", "êµ¬ì²´ì "]
        rag_has_context = any(k in rag_response for k in context_keywords)
        no_rag_has_context = any(k in no_rag_response for k in context_keywords)

        # 4. êµ¬ì²´ì„± ì ìˆ˜ ê³„ì‚°
        rag_specificity = self._calculate_specificity_score(rag_response, query)
        no_rag_specificity = self._calculate_specificity_score(no_rag_response, query)

        # 5. í’ˆì§ˆ í–¥ìƒ íŒì • (ê°œì„ : ì„ê³„ê°’ 0.1 â†’ 0.15ë¡œ ì™„í™”í•˜ì—¬ ë…¸ì´ì¦ˆ ê°ì†Œ)
        improvement_score = rag_specificity - no_rag_specificity

        if improvement_score > 0.15:
            quality_improvement = "better"
        elif improvement_score < -0.15:
            quality_improvement = "worse"
        else:
            quality_improvement = "same"

        return ComparisonMetrics(
            query=query,
            rag_response=rag_response,
            rag_time_ms=rag_time,
            rag_is_question=rag_is_question,
            rag_has_context=rag_has_context,
            rag_specificity_score=rag_specificity,
            no_rag_response=no_rag_response,
            no_rag_time_ms=no_rag_time,
            no_rag_is_question=no_rag_is_question,
            no_rag_has_context=no_rag_has_context,
            no_rag_specificity_score=no_rag_specificity,
            quality_improvement=quality_improvement,
            improvement_score=improvement_score
        )

    def identify_best_worst_cases(
        self,
        results: List[ComparisonMetrics]
    ) -> Dict:
        """
        Best/Worst ì¼€ì´ìŠ¤ ì‹ë³„

        Args:
            results: ë¹„êµ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸

        Returns:
            Best/Worst ì¼€ì´ìŠ¤ ì •ë³´
        """
        if not results:
            return {"best": None, "worst": None}

        # improvement_score ê¸°ì¤€ ì •ë ¬
        sorted_results = sorted(
            results,
            key=lambda x: x.improvement_score,
            reverse=True
        )

        return {
            "best": sorted_results[0] if sorted_results else None,
            "worst": sorted_results[-1] if sorted_results else None
        }

    def run_comparison_evaluation(
        self,
        n_samples: int = 5,
        save_results: bool = True,
        output_dir: Optional[Path] = None
    ) -> Tuple[List[ComparisonMetrics], Dict]:
        """
        RAG vs non-RAG ë¹„êµ í‰ê°€ ì‹¤í–‰

        Args:
            n_samples: í…ŒìŠ¤íŠ¸í•  ìƒ˜í”Œ ìˆ˜
            save_results: ê²°ê³¼ ì €ì¥ ì—¬ë¶€
            output_dir: ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬

        Returns:
            (ë¹„êµ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸, í†µê³„ ë° Best/Worst ì¼€ì´ìŠ¤)
        """
        print(f"\n{'='*60}")
        print(f"RAG vs non-RAG ë¹„êµ í‰ê°€ ì‹œì‘ (ìƒ˜í”Œ ìˆ˜: {n_samples})")
        print(f"{'='*60}\n")

        test_queries = self.create_test_queries(n_samples)
        comparison_results = []

        for i, tc in enumerate(test_queries):
            print(f"[{i+1}/{len(test_queries)}] {tc['query'][:40]}...")
            metrics = self.evaluate_comparison(tc["query"])
            comparison_results.append(metrics)
            print(f"  â†’ RAG: {metrics.rag_specificity_score:.2f} | non-RAG: {metrics.no_rag_specificity_score:.2f} | ê²°ê³¼: {metrics.quality_improvement}")

        # í†µê³„ ê³„ì‚°
        better_count = sum(1 for r in comparison_results if r.quality_improvement == "better")
        same_count = sum(1 for r in comparison_results if r.quality_improvement == "same")
        worse_count = sum(1 for r in comparison_results if r.quality_improvement == "worse")

        best_worst = self.identify_best_worst_cases(comparison_results)

        stats = {
            "total": len(comparison_results),
            "better_count": better_count,
            "better_rate": better_count / len(comparison_results) if comparison_results else 0,
            "same_count": same_count,
            "same_rate": same_count / len(comparison_results) if comparison_results else 0,
            "worse_count": worse_count,
            "worse_rate": worse_count / len(comparison_results) if comparison_results else 0,
            "avg_improvement": sum(r.improvement_score for r in comparison_results) / len(comparison_results) if comparison_results else 0
        }

        # ê²°ê³¼ ì¶œë ¥
        self._print_comparison_summary(stats, best_worst)

        # ê²°ê³¼ ì €ì¥
        if save_results:
            self._save_comparison_results(comparison_results, stats, best_worst, output_dir)

        return comparison_results, {"stats": stats, "best_worst": best_worst}

    def _print_comparison_summary(self, stats: Dict, best_worst: Dict):
        """ë¹„êµ í‰ê°€ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print(f"\n{'='*60}")
        print("ğŸ“Š RAG vs non-RAG ë¹„êµ í‰ê°€ ê²°ê³¼")
        print(f"{'='*60}")

        print(f"\nğŸ“ˆ ì „ì²´ ë¹„êµ:")
        print(f"  - RAGê°€ ë” ë‚˜ì€ ê²½ìš°: {stats['better_count']}ê±´ ({stats['better_rate']*100:.1f}%)")
        print(f"  - ë™ì¼í•œ ê²½ìš°: {stats['same_count']}ê±´ ({stats['same_rate']*100:.1f}%)")
        print(f"  - RAGê°€ ë” ë‚˜ìœ ê²½ìš°: {stats['worse_count']}ê±´ ({stats['worse_rate']*100:.1f}%)")
        print(f"  - í‰ê·  í–¥ìƒ ì ìˆ˜: {stats['avg_improvement']:.3f}")

        if best_worst.get("best"):
            best = best_worst["best"]
            print(f"\nğŸ† Best Case (RAG íš¨ê³¼ ìµœëŒ€, í–¥ìƒ: +{best.improvement_score:.2f}):")
            print(f"  Query: \"{best.query[:50]}...\"")
            print(f"\n  [RAG ì‘ë‹µ] (êµ¬ì²´ì„±: {best.rag_specificity_score:.2f})")
            print(f"  \"{best.rag_response}\"")
            print(f"\n  [non-RAG ì‘ë‹µ] (êµ¬ì²´ì„±: {best.no_rag_specificity_score:.2f})")
            print(f"  \"{best.no_rag_response}\"")

        if best_worst.get("worst"):
            worst = best_worst["worst"]
            print(f"\nğŸ“‰ Worst Case (RAG íš¨ê³¼ ë¯¸ë¯¸, í–¥ìƒ: {worst.improvement_score:.2f}):")
            print(f"  Query: \"{worst.query[:50]}...\"")
            print(f"\n  [RAG ì‘ë‹µ] (êµ¬ì²´ì„±: {worst.rag_specificity_score:.2f})")
            print(f"  \"{worst.rag_response}\"")
            print(f"\n  [non-RAG ì‘ë‹µ] (êµ¬ì²´ì„±: {worst.no_rag_specificity_score:.2f})")
            print(f"  \"{worst.no_rag_response}\"")

        print(f"\n{'='*60}\n")

    def _save_comparison_results(
        self,
        results: List[ComparisonMetrics],
        stats: Dict,
        best_worst: Dict,
        output_dir: Optional[Path] = None
    ):
        """ë¹„êµ í‰ê°€ ê²°ê³¼ ì €ì¥"""
        if output_dir is None:
            output_dir = Path(__file__).parent / "evaluation_results"

        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = output_dir / f"comparison_{timestamp}.json"

        data = {
            "timestamp": datetime.now().isoformat(),
            "stats": stats,
            "best_case": asdict(best_worst["best"]) if best_worst.get("best") else None,
            "worst_case": asdict(best_worst["worst"]) if best_worst.get("worst") else None,
            "all_results": [asdict(r) for r in results]
        }

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        print(f"ğŸ’¾ ë¹„êµ ê²°ê³¼ ì €ì¥ë¨: {output_file}")

    # =========================================================================
    # Hybrid Evaluation (ContextScorer ê¸°ë°˜)
    # =========================================================================

    async def evaluate_hybrid_comparison(
        self,
        query: str,
        context_threshold: float = 0.35
    ) -> HybridComparisonMetrics:
        """
        ContextScorer ê¸°ë°˜ Hybrid ë¹„êµ í‰ê°€

        ê¸°ì¡´ evaluate_comparison()ê³¼ ë‹¬ë¦¬:
        - ë¹„ë™ê¸° ì‹¤í–‰ (RAG/non-RAG ë³‘ë ¬)
        - ContextScorerë¡œ RAG ì‘ë‹µì˜ ì»¨í…ìŠ¤íŠ¸ ì°¸ì¡°ìœ¨ ì¸¡ì •
        - ìë™ ì‘ë‹µ ì„ íƒ ë¡œì§ í¬í•¨

        Args:
            query: í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
            context_threshold: ì»¨í…ìŠ¤íŠ¸ ì°¸ì¡° ì„ê³„ê°’ (ê¸°ë³¸: 0.35)

        Returns:
            HybridComparisonMetrics: Hybrid ë¹„êµ ê²°ê³¼
        """
        from .async_chain import HybridRAGGenerator
        from .vectorstore import get_embeddings

        # HybridRAGGenerator ì‚¬ìš©
        embedding_model = get_embeddings()
        generator = HybridRAGGenerator(
            vectorstore=self.rag.vectorstore,
            k=self.rag.k,
            model=self.rag.model,
            temperature=self.rag.temperature,
            context_threshold=context_threshold,
            embedding_model=embedding_model
        )

        start_time = time.time()
        _, metadata = await generator.generate_hybrid(query)
        total_time = (time.time() - start_time) * 1000

        return HybridComparisonMetrics(
            query=query,
            rag_response=metadata["rag_response"],
            rag_time_ms=total_time / 2,  # ë³‘ë ¬ì´ë¯€ë¡œ ì¶”ì •
            rag_context_score=metadata["context_score"],
            rag_token_overlap=metadata["score_details"]["token_overlap"],
            rag_doc_reference=metadata["score_details"]["doc_reference"],
            rag_semantic_similarity=metadata["score_details"]["semantic_similarity"],
            rag_referenced_doc_count=metadata["score_details"].get("referenced_doc_count", 0),
            no_rag_response=metadata["no_rag_response"],
            no_rag_time_ms=total_time / 2,
            selected_source=metadata["source"],
            context_threshold=context_threshold,
            retrieved_docs=metadata.get("retrieved_docs", [])
        )

    async def run_hybrid_evaluation(
        self,
        n_samples: int = 5,
        context_threshold: float = 0.35,
        save_results: bool = True,
        output_dir: Optional[Path] = None
    ) -> Tuple[List[HybridComparisonMetrics], Dict]:
        """
        Hybrid RAG í‰ê°€ ì‹¤í–‰

        ContextScorer ê¸°ë°˜ìœ¼ë¡œ RAG ì‘ë‹µì˜ í’ˆì§ˆì„ í‰ê°€

        Args:
            n_samples: í…ŒìŠ¤íŠ¸í•  ìƒ˜í”Œ ìˆ˜
            context_threshold: ì»¨í…ìŠ¤íŠ¸ ì°¸ì¡° ì„ê³„ê°’
            save_results: ê²°ê³¼ ì €ì¥ ì—¬ë¶€
            output_dir: ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬

        Returns:
            (ë¹„êµ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸, í†µê³„)
        """
        print(f"\n{'='*60}")
        print(f"Hybrid RAG í‰ê°€ ì‹œì‘ (ContextScorer ê¸°ë°˜, ì„ê³„ê°’: {context_threshold})")
        print(f"{'='*60}\n")

        test_queries = self.create_test_queries(n_samples)
        results = []

        for i, tc in enumerate(test_queries):
            print(f"[{i+1}/{len(test_queries)}] \"{tc['query'][:40]}...\"")

            metrics = await self.evaluate_hybrid_comparison(
                tc["query"],
                context_threshold=context_threshold
            )
            results.append(metrics)

            print(f"  â†’ Context Score: {metrics.rag_context_score:.2f} "
                  f"(token: {metrics.rag_token_overlap:.2f}, "
                  f"doc: {metrics.rag_doc_reference:.2f}, "
                  f"semantic: {metrics.rag_semantic_similarity:.2f})")
            print(f"  â†’ Selected: {metrics.selected_source} | "
                  f"Referenced Docs: {metrics.rag_referenced_doc_count}/3")

        # í†µê³„ ê³„ì‚°
        stats = self._calculate_hybrid_stats(results)

        # ê²°ê³¼ ì¶œë ¥
        self._print_hybrid_summary(results, stats)

        # ê²°ê³¼ ì €ì¥
        if save_results:
            self._save_hybrid_results(results, stats, output_dir)

        return results, stats

    def _calculate_hybrid_stats(
        self,
        results: List[HybridComparisonMetrics]
    ) -> Dict:
        """
        Hybrid í‰ê°€ í†µê³„ ê³„ì‚°

        Args:
            results: Hybrid ë¹„êµ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸

        Returns:
            í†µê³„ ë”•ì…”ë„ˆë¦¬
        """
        if not results:
            return {}

        rag_count = sum(1 for r in results if r.selected_source == "RAG")
        no_rag_count = len(results) - rag_count

        avg_context_score = sum(r.rag_context_score for r in results) / len(results)
        avg_token_overlap = sum(r.rag_token_overlap for r in results) / len(results)
        avg_doc_reference = sum(r.rag_doc_reference for r in results) / len(results)
        avg_semantic_similarity = sum(r.rag_semantic_similarity for r in results) / len(results)
        avg_referenced_docs = sum(r.rag_referenced_doc_count for r in results) / len(results)

        # Best/Worst ì¼€ì´ìŠ¤
        sorted_results = sorted(results, key=lambda x: x.rag_context_score, reverse=True)
        best_case = sorted_results[0] if sorted_results else None
        worst_case = sorted_results[-1] if sorted_results else None

        return {
            "total": len(results),
            "rag_count": rag_count,
            "rag_rate": rag_count / len(results),
            "no_rag_count": no_rag_count,
            "no_rag_rate": no_rag_count / len(results),
            "avg_context_score": avg_context_score,
            "avg_token_overlap": avg_token_overlap,
            "avg_doc_reference": avg_doc_reference,
            "avg_semantic_similarity": avg_semantic_similarity,
            "avg_referenced_docs": avg_referenced_docs,
            "best_case": best_case,
            "worst_case": worst_case
        }

    def _print_hybrid_summary(
        self,
        _results: List[HybridComparisonMetrics],
        stats: Dict
    ):
        """Hybrid í‰ê°€ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print(f"\n{'='*60}")
        print("ğŸ“Š Hybrid RAG í‰ê°€ ê²°ê³¼")
        print(f"{'='*60}")

        print(f"\nğŸ“ˆ ì „ì²´ í†µê³„:")
        print(f"  - í‰ê·  Context Score: {stats['avg_context_score']:.2f}")
        print(f"  - RAG ì„ íƒ ë¹„ìœ¨: {stats['rag_rate']*100:.1f}% ({stats['rag_count']}/{stats['total']})")
        print(f"  - non-RAG ì„ íƒ ë¹„ìœ¨: {stats['no_rag_rate']*100:.1f}% ({stats['no_rag_count']}/{stats['total']})")

        print(f"\nğŸ“Š ì„¸ë¶€ ë©”íŠ¸ë¦­:")
        print(f"  - í‰ê·  í† í° ì˜¤ë²„ë©: {stats['avg_token_overlap']:.2f}")
        print(f"  - í‰ê·  ë¬¸ì„œ ì°¸ì¡°ìœ¨: {stats['avg_doc_reference']:.2f}")
        print(f"  - í‰ê·  ì˜ë¯¸ì  ìœ ì‚¬ë„: {stats['avg_semantic_similarity']:.2f}")
        print(f"  - í‰ê·  ì°¸ì¡° ë¬¸ì„œ ìˆ˜: {stats['avg_referenced_docs']:.1f}")

        if stats.get("best_case"):
            best = stats["best_case"]
            print(f"\nğŸ† Best Case (Context Score ìµœê³ ):")
            print(f"  Query: \"{best.query[:50]}...\"")
            print(f"  Score: {best.rag_context_score:.2f} â†’ {best.selected_source} ì„ íƒ")
            print(f"  RAG ì‘ë‹µ: \"{best.rag_response[:80]}...\"")

        if stats.get("worst_case"):
            worst = stats["worst_case"]
            print(f"\nğŸ“‰ Worst Case (Context Score ìµœì €):")
            print(f"  Query: \"{worst.query[:50]}...\"")
            print(f"  Score: {worst.rag_context_score:.2f} â†’ {worst.selected_source} ì„ íƒ")
            print(f"  RAG ì‘ë‹µ: \"{worst.rag_response[:80]}...\"")

        print(f"\n{'='*60}\n")

    def _save_hybrid_results(
        self,
        results: List[HybridComparisonMetrics],
        stats: Dict,
        output_dir: Optional[Path] = None
    ):
        """Hybrid í‰ê°€ ê²°ê³¼ ì €ì¥"""
        if output_dir is None:
            output_dir = Path(__file__).parent / "evaluation_results"

        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = output_dir / f"hybrid_{timestamp}.json"

        # statsì—ì„œ best_case, worst_case ì œê±° (ë³„ë„ ì²˜ë¦¬)
        stats_copy = {k: v for k, v in stats.items() if k not in ["best_case", "worst_case"]}

        data = {
            "timestamp": datetime.now().isoformat(),
            "stats": stats_copy,
            "best_case": asdict(stats["best_case"]) if stats.get("best_case") else None,
            "worst_case": asdict(stats["worst_case"]) if stats.get("worst_case") else None,
            "all_results": [asdict(r) for r in results]
        }

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        print(f"ğŸ’¾ Hybrid ê²°ê³¼ ì €ì¥ë¨: {output_file}")


def run_quick_test():
    """ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ (3ê°œ ìƒ˜í”Œ)"""
    evaluator = RAGEvaluator()
    return evaluator.run_evaluation(n_samples=3, include_generation=True)


def run_full_evaluation():
    """ì „ì²´ í‰ê°€ (20ê°œ ìƒ˜í”Œ)"""
    evaluator = RAGEvaluator()
    return evaluator.run_evaluation(n_samples=20, include_generation=True)


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="RAG ì‹¤íš¨ì„± í‰ê°€")
    parser.add_argument(
        "--samples", "-n",
        type=int,
        default=10,
        help="í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ìˆ˜ (ê¸°ë³¸: 10)"
    )
    parser.add_argument(
        "--quick", "-q",
        action="store_true",
        help="ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ (3ê°œ ìƒ˜í”Œ)"
    )
    parser.add_argument(
        "--no-generation",
        action="store_true",
        help="ìƒì„± í‰ê°€ ì œì™¸ (ê²€ìƒ‰ë§Œ í‰ê°€)"
    )
    parser.add_argument(
        "--no-save",
        action="store_true",
        help="ê²°ê³¼ ì €ì¥ ì•ˆ í•¨"
    )
    parser.add_argument(
        "--compare", "-c",
        action="store_true",
        help="RAG vs non-RAG ë¹„êµ í‰ê°€ ëª¨ë“œ"
    )
    parser.add_argument(
        "--hybrid",
        action="store_true",
        help="Hybrid í‰ê°€ ëª¨ë“œ (ContextScorer ê¸°ë°˜)"
    )
    parser.add_argument(
        "--threshold", "-t",
        type=float,
        default=0.5,
        help="Context score ì„ê³„ê°’ (ê¸°ë³¸: 0.5)"
    )

    args = parser.parse_args()

    if args.hybrid:
        # Hybrid í‰ê°€ ëª¨ë“œ (ContextScorer ê¸°ë°˜)
        evaluator = RAGEvaluator()
        asyncio.run(evaluator.run_hybrid_evaluation(
            n_samples=args.samples if not args.quick else 3,
            context_threshold=args.threshold,
            save_results=not args.no_save
        ))
    elif args.compare:
        # ë¹„êµ í‰ê°€ ëª¨ë“œ
        evaluator = RAGEvaluator()
        evaluator.run_comparison_evaluation(
            n_samples=args.samples if not args.quick else 3,
            save_results=not args.no_save
        )
    elif args.quick:
        run_quick_test()
    else:
        evaluator = RAGEvaluator()
        evaluator.run_evaluation(
            n_samples=args.samples,
            include_generation=not args.no_generation,
            save_results=not args.no_save
        )
