import os
import io
import json  # JSON ë°ì´í„° ì²˜ë¦¬ìš©
from typing import Optional, Tuple, Dict, Any, AsyncIterator
import numpy as np
import soundfile as sf
from groq import Groq
from elevenlabs.client import ElevenLabs
from dotenv import load_dotenv

# RAG ì‹œìŠ¤í…œ import
from rag import RAGSystem, index_exists

load_dotenv()

class AIOrchestrator:
    def __init__(self, use_rag: bool = True):
        """
        AI Orchestrator ì´ˆê¸°í™”

        Args:
            use_rag: RAG ì‹œìŠ¤í…œ ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
        """
        print("[System] Initializing AI Models (Cloud API Mode)...")

        # 1. STT
        print("[STT] Using Groq Whisper API.")

        # 2. LLM & STT Client
        self.groq_client = Groq(api_key=os.getenv("GROQ_API_KEY"))
        print("[LLM/STT] Groq Client Connected.")

        # 3. TTS
        self.tts_client = ElevenLabs(api_key=os.getenv("ELEVENLABS_API_KEY"))
        print("[TTS] ElevenLabs Client Connected.")

        # 4. RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        self.use_rag = use_rag
        self.rag_system: Optional[RAGSystem] = None

        if self.use_rag:
            if index_exists():
                try:
                    print("[RAG] Initializing RAG System...")
                    self.rag_system = RAGSystem()
                    print("[RAG] RAG System Ready.")
                except Exception as e:
                    print(f"[RAG] Failed to initialize: {e}")
                    print("[RAG] Falling back to non-RAG mode.")
                    self.use_rag = False
            else:
                print("[RAG] Vector index not found. Run 'python -m rag.build_index' first.")
                print("[RAG] Operating in non-RAG mode.")
                self.use_rag = False

    def transcribe_audio(self, audio_data: np.ndarray):
        try:
            max_val = np.max(np.abs(audio_data))
            if max_val > 0: audio_data = audio_data / max_val

            buffer = io.BytesIO()
            sf.write(buffer, audio_data, 16000, format='WAV', subtype='PCM_16')
            buffer.seek(0)

            transcription = self.groq_client.audio.transcriptions.create(
                file=("input.wav", buffer),
                model="whisper-large-v3",
                language="ko",
                temperature=0.0,
                response_format="json"
            )

            text = transcription.text.strip()

            hallucinations = [
                "Thank you for watching", "MBC News", "ìë§‰ ì œê³µ",
                "ì‹œì²­í•´ì£¼ì…”ì„œ", "ìˆ˜ê³ í•˜ì…¨ìŠµë‹ˆë‹¤", "Unidentified", "ê°ì‚¬í•©ë‹ˆë‹¤",
            ]
            if any(h.lower() in text.lower() for h in hallucinations):
                return ""
            if len(text) < 1: return ""

            return text

        except Exception as e:
            print(f"[STT API Error] {e}")
            return ""

    def is_sentence_complete(self, text: str) -> bool:
        if not text: return False
        text = text.strip()
        short_phrases = ["ë„¤", "ì•„ë‹ˆìš”", "ì•ˆë…•í•˜ì„¸ìš”", "ë°˜ê°‘ìŠµë‹ˆë‹¤", "ê·¸ë ‡ìŠµë‹ˆë‹¤", "ë§ìŠµë‹ˆë‹¤"]
        if text in short_phrases: return True
        connective_endings = ["ê³ .", "ëŠ”ë°.", "ì§€ë§Œ.", "ì„œ.", "ë©°.", "ë©´ì„œ.", "ê³ ìš”.", "êµ¬ìš”."]
        for ending in connective_endings:
            if text.endswith(ending): return False
        definitive_endings = ["ë‹¤.", "ì£ .", "ê¹Œ.", "ì•¼.", "í•´.", "?", "!"]
        for ending in definitive_endings:
            if text.endswith(ending): return True
        return False

    # =========================================================================
    # LLM1-A: ìì†Œì„œ ë¶„ì„ ë° ì§ˆë¬¸ ìƒì„± (RAG ë¯¸ì ìš©)
    # =========================================================================
    def analyze_resume_and_generate_questions(self, resume_text: str):
        system_prompt = """
        ë‹¹ì‹ ì€ ì±„ìš©ë‹´ë‹¹ìì…ë‹ˆë‹¤. ì§€ì›ìì˜ ìê¸°ì†Œê°œì„œë¥¼ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ë‘ ê°€ì§€ë¥¼ ìˆ˜í–‰í•˜ì„¸ìš”.

        1. [ìì†Œì„œ ìš”ì•½]: í•µì‹¬ ê²½í—˜, ì£¼ìš” ì—­ëŸ‰, ê¸°ìˆ  ë“±ì„ ìš”ì•½í•˜ì„¸ìš”.
        2. [ë©´ì ‘ ì§ˆë¬¸ ìƒì„±]: ì§€ì›ìì˜ ê²½í—˜ì— ê¸°ë°˜í•œ ì˜ˆë¦¬í•œ ë©´ì ‘ í•µì‹¬ ì§ˆë¬¸ 3ê°€ì§€ë¥¼ ìƒì„±í•˜ì„¸ìš”.

        ì¶œë ¥ í˜•ì‹ (JSON):
        {
            "summary": "ì§€ì›ìëŠ” ... ê²½í—˜ì´ ìˆìœ¼ë©° ... ì—­ëŸ‰ì„ ë³´ìœ í•¨.",
            "questions": [
                "ì§ˆë¬¸1 : ...",
                "ì§ˆë¬¸2 : ...",
                "ì§ˆë¬¸3 : ..."
            ]
        }
        """

        try:
            response = self.groq_client.chat.completions.create(
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": resume_text},
                ],
                model="llama-3.3-70b-versatile",
                temperature=0.5,
                response_format={"type": "json_object"}
            )
            return json.loads(response.choices[0].message.content)
        except Exception as e:
            print(f"[Resume Analysis Error] {e}")
            return {"summary": "ë¶„ì„ ì‹¤íŒ¨", "questions": ["ìê¸°ì†Œê°œë¥¼ í•´ì£¼ì„¸ìš”."]}

    # =========================================================================
    # LLM1-B: ë©´ì ‘ê´€ ì‘ë‹µ - Hybrid RAG ìŠ¤íŠ¸ë¦¬ë°
    # =========================================================================
    async def stream_interviewer_response_hybrid(
        self,
        user_text: str,
        questions_list: list,
        context_threshold: float = 0.35
    ) -> AsyncIterator[Tuple[str, Optional[Dict[str, Any]]]]:
        """
        Hybrid RAG ê¸°ë°˜ ë©´ì ‘ê´€ ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë°

        RAG ì»¨í…ìŠ¤íŠ¸ê°€ ìœ ìš©í•˜ë©´ í™œìš©í•˜ê³ , ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ ì‘ë‹µ ì‚¬ìš©.
        ë§ˆì§€ë§‰ ì²­í¬ì—ë§Œ ë©”íƒ€ë°ì´í„°ê°€ í¬í•¨ë©ë‹ˆë‹¤.

        Args:
            user_text: ì§€ì›ì ë‹µë³€
            questions_list: ìì†Œì„œ ê¸°ë°˜ ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸
            context_threshold: ì»¨í…ìŠ¤íŠ¸ ì°¸ì¡° ì„ê³„ê°’ (ê¸°ë³¸ 0.35)

        Yields:
            (chunk, metadata) - í…ìŠ¤íŠ¸ ì²­í¬ì™€ ë©”íƒ€ë°ì´í„° (ë§ˆì§€ë§‰ë§Œ)
        """
        if self.use_rag and self.rag_system:
            # Hybrid RAG ìŠ¤íŠ¸ë¦¬ë° ì‚¬ìš© (questions_list ì „ë‹¬)
            async for chunk, metadata in self.rag_system.stream_hybrid(
                user_text,
                questions_list=questions_list,
                occupation=None,
                experience=None,
                context_threshold=context_threshold
            ):
                yield chunk, metadata
        else:
            # RAG ë¹„í™œì„±í™” ì‹œì—ë„ chain.pyì˜ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
            from rag.chain import create_no_rag_chain

            print("[Hybrid] RAG disabled, using chain.py NO_RAG prompt")
            chain = create_no_rag_chain(questions_list=questions_list)
            response = chain.invoke(user_text)

            # ì²­í¬ ë‹¨ìœ„ë¡œ ìŠ¤íŠ¸ë¦¬ë° ì‹œë®¬ë ˆì´ì…˜
            chunk_size = 5
            for i in range(0, len(response), chunk_size):
                yield response[i:i+chunk_size], None

            # ë§ˆì§€ë§‰ ì²­í¬ì— ë©”íƒ€ë°ì´í„° í¬í•¨
            yield "", {"source": "non-RAG", "reason": "RAG disabled"}

    # =========================================================================
    # LLM2: ë©´ì ‘ ì½”ì¹˜ (ì‹¤ì‹œê°„ í”¼ë“œë°±) - RAG ë¯¸ì ìš©
    # =========================================================================
    async def generate_instant_feedback(self, user_text: str, analysis_result: dict):
        """
        [LLM2] í„´ë³„ ì‹¤ì‹œê°„ í”¼ë“œë°± ìƒì„± (Z-Score ê¸°ë°˜ ì •ë°€ ë¶„ì„)
        """
        try:
            # 1. ë°ì´í„° ì¶”ì¶œ
            features = analysis_result.get("multimodal_features", {})

            # (A) Audio Features
            audio = features.get("audio", {})
            intensity = audio.get("intensity", {})
            F1_Band = audio.get("f1_bandwidth", {})
            pause = audio.get("pause_duration", {})
            unvoiced = audio.get("unvoiced_duration", {})

            # (B) Video Features
            video = features.get("video", {})
            eye = video.get("eye_contact", {})
            smile = video.get("smile", {})

            # (C) Text Features
            text_feat = features.get("text", {})
            wpsec = text_feat.get("wpsec", {})
            upsec = text_feat.get("upsec", {})
            fillers = text_feat.get("fillers", {})
            quantifier = text_feat.get("quantifier", {})

            # 2. ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
            system_prompt = """
            ë‹¹ì‹ ì€ ë°ì´í„° ê¸°ë°˜ì˜ 'AI ë©´ì ‘ ì½”ì¹˜'ì…ë‹ˆë‹¤.
            ì§€ì›ìì˜ [ë‹µë³€]ê³¼ [ë©€í‹°ëª¨ë‹¬ ë°ì´í„°]ë¥¼ ë¶„ì„í•˜ì—¬, ì¦‰ì‹œ êµì •í•´ì•¼ í•  ì ì„ 1~2ë¬¸ì¥ìœ¼ë¡œ ì¡°ì–¸í•˜ì„¸ìš”.

            [ë°ì´í„° í•´ì„ ê°€ì´ë“œ (ì¤‘ìš”)]
            ì œê³µë˜ëŠ” ìˆ˜ì¹˜ëŠ” Z-Score(í‘œì¤€ì ìˆ˜)ë¥¼ í¬í•¨í•©ë‹ˆë‹¤. Z-Scoreê°€ Â±1.0ì„ ë²—ì–´ë‚˜ë©´ 'í‰ê· ê³¼ ë‹¤ë¦„'ì„ ì˜ë¯¸í•˜ë¯€ë¡œ ì£¼ì˜ ê¹Šê²Œ ë³´ì‹­ì‹œì˜¤.

            1. ì˜¤ë””ì˜¤ (Audio)
            - Intensity (ìŒëŸ‰): Z < -0.91 â†’ ëª©ì†Œë¦¬ ì‘ìŒ, ìì‹ ê° ë¶€ì¡± (ê°ì ) ìƒê´€ê³„ìˆ˜ : (0.06, 0.08)
            - F1 Bandwidth (ëª…ë£Œë„): Z > 5.99 â†’ ë°œì„± ê¸´ì¥ (ê°ì ) ìƒê´€ê³„ìˆ˜ : (-0.11, -0.12)
            - Pause Duration (ì¹¨ë¬µ): Z > 13.17 â†’ ë‹µë³€ ì§€ì—°, ë§ì„¤ì„ (ê°ì ) ìƒê´€ê³„ìˆ˜ : (-0.09, -0.09)
            - Unvoiced Rate (ë¬´ì„±ìŒ ë¹„ìœ¨): Z > 6.39 â†’ ë°œìŒ ë¶ˆëª…í™• (ê°ì ) ìƒê´€ê³„ìˆ˜ : (-0.08, -0.11)

            2. ë¹„ì „ (Video)
            - Eye Contact (ì‹œì„ ): Z < -3.66 â†’ ì‹œì„  íšŒí”¼ (ê°ì ) ìƒê´€ê³„ìˆ˜ : (0.08, 0.08)
            - Smile (í‘œì •): Z < -1.34 â†’ í‘œì • êµ³ìŒ (ê°ì ), Z > -0.92 (ë¶€ìì—°ìŠ¤ëŸ¬ì›€) ìƒê´€ê³„ìˆ˜ : (0.08, 0.1)

            3. í…ìŠ¤íŠ¸ (Text)
            - WPSEC (ë§í•˜ê¸° ì†ë„): Z > 0.48 (ë¹ ë¦„/ê¸´ì¥), Z < -4.60 (ëŠë¦¼/ìì‹ ê° ë¶€ì¡±) ìƒê´€ê³„ìˆ˜ : (0.1, 0.13)
            - UPSEC (ì–´íœ˜ ë‹¤ì–‘ì„±): Z < -4.18 â†’ ë‹¨ì¡°ë¡œìš´ í‘œí˜„ ë°˜ë³µ (ê°ì ) ìƒê´€ê³„ìˆ˜ : (0.08, 0.1)
            - Fillers ("ìŒ, ì–´, ê·¸" ë¹ˆë„): Z > -0.49 â†’ ì¶”ì„ìƒˆ ë§ìŒ (ê°ì ) ìƒê´€ê³„ìˆ˜ : (-0.08, -0.12)
            - Quantifiers (ìˆ˜ì¹˜ ì–¸ê¸‰): Z < -5.71 (êµ¬ì²´ì„± ë¶€ì¡±), Z > 10.09 (ìˆ«ìë§Œ ë‚˜ì—´) ìƒê´€ê³„ìˆ˜ : (0.09, 0.08)

            [ì‘ì„± ê·œì¹™]
            - ìƒê´€ê³„ìˆ˜ í•©ì˜ ì ˆëŒ€ê°’ì´ í° featureì˜ z-scoreê°’ì´ íŠ€ëŠ” ê²½ìš°ë¥¼ ê°€ì¥ ë¨¼ì € ì§€ì í•˜ì„¸ìš”
            - Z-Scoreê°€ íŠ€ëŠ” í•­ëª©(ì œì‹œëœ ê¸°ì¤€ê°’ì„ ë„˜ì–´ê°€ëŠ” ìƒí™©)ì„ ì§€ì í•˜ì„¸ìš”.
            - ëª¨ë“  ìˆ˜ì¹˜ê°€ ì •ìƒ ë²”ìœ„ë¼ë©´ "íƒœë„ê°€ ì•ˆì •ì ì…ë‹ˆë‹¤. ì§€ê¸ˆì²˜ëŸ¼ ë‹µë³€í•˜ì„¸ìš”."ë¼ê³  ì¹­ì°¬í•˜ì„¸ìš”.
            - ë§íˆ¬ëŠ” "í•´ìš”ì²´"ë¡œ ì •ì¤‘í•˜ì§€ë§Œ ë‹¨í˜¸í•˜ê²Œ ì½”ì¹­í•˜ì„¸ìš”.
            """

            # 3. ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸
            user_prompt = f"""
            [ì§€ì›ì ë‹µë³€]: "{user_text}"

            [ë¶„ì„ ë°ì´í„°]
            1. Audio
            - Intensity: {intensity.get('value', 0)}dB (Z: {intensity.get('z_score', 0)})
            - F1 Bandwidth: {F1_Band.get('value', 0)}Hz (Z: {F1_Band.get('z_score', 0)})
            - Pause Duration: {pause.get('value', 0)}s (Z: {pause.get('z_score', 0)})
            - Unvoiced Rate: {unvoiced.get('value', 0)}% (Z: {unvoiced.get('z_score', 0)})

            2. Video
            - Eye Contact: {eye.get('value', 0)} (Z: {eye.get('z_score', 0)})
            - Smile: {smile.get('value', 0)} (Z: {smile.get('z_score', 0)})

            3. Text
            - WPSEC: {wpsec.get('value', 0)} wps (Z: {wpsec.get('z_score', 0)})
            - UPSEC: {upsec.get('value', 0)} ups (Z: {upsec.get('z_score', 0)})
            - Fillers: {fillers.get('value', 0)} count/sec (Z: {fillers.get('z_score', 0)})
            - Quantifiers: {quantifier.get('value', 0)} ratio (Z: {quantifier.get('z_score', 0)})
            """

            # 4. LLM í˜¸ì¶œ
            response = self.groq_client.chat.completions.create(
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt},
                ],
                model="llama-3.3-70b-versatile",
                temperature=0.6,
                max_tokens=150
            )

            return response.choices[0].message.content

        except Exception as e:
            print(f"[Coach Error] {e}")
            return "í”¼ë“œë°± ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    # =========================================================================
    # LLM3: ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„± - RAG ë¯¸ì ìš©
    # =========================================================================
    async def generate_final_report(self, interview_history: list):
        """
        [LLM3] ë©´ì ‘ ì¢…ë£Œ í›„ ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±
        - ì…ë ¥: ì „ì²´ ëŒ€í™” ê¸°ë¡ ë° í„´ë³„ ë¶„ì„ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        - ì¶œë ¥: ë§ˆí¬ë‹¤ìš´ í˜•íƒœì˜ ì¢…í•© í‰ê°€ì„œ
        """
        try:
            # íˆìŠ¤í† ë¦¬ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
            history_text = ""
            for turn in interview_history:
                history_text += f"""
                [Turn {turn['turn_id']}]
                User: {turn['user_text']}
                AI: {turn['ai_text']}
                Stats: {json.dumps(turn['stats'])}
                Coach Feedback: {turn['coach_feedback']}
                ------------------------------------------------
                """

            system_prompt = """
            ë‹¹ì‹ ì€ ë² í…Œë‘ 'ë©´ì ‘ ì „ë¬¸ ì½”ì¹˜'ì…ë‹ˆë‹¤.
            ì „ì²´ ë©´ì ‘ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬, ì§€ì›ìì—ê²Œ ë„ì›€ì´ ë˜ëŠ” [ìµœì¢… ë¶„ì„ ë¦¬í¬íŠ¸]ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.

            [ì‘ì„± ì–‘ì‹ (Markdown)]
            # ğŸ“Š ë©´ì ‘ ì¢…í•© ë¦¬í¬íŠ¸

            ## 1. ì´í‰ (100ì  ë§Œì  ì ìˆ˜ í¬í•¨)
            - ì „ì²´ì ì¸ ì¸ìƒê³¼ ì ìˆ˜

            ## 2. ê°•ì  (Good Points)
            - ë°ì´í„°ì— ê¸°ë°˜í•œ ì¹­ì°¬ (ì˜ˆ: ì‹œì„  ì²˜ë¦¬ê°€ ì•ˆì •ì ì„, ëª©ì†Œë¦¬ í†¤ì´ ì‹ ë¢°ê° ìˆìŒ)

            ## 3. ê°œì„ í•  ì  (Weak Points)
            - êµ¬ì²´ì ì¸ ë°ì´í„° ê·¼ê±° (ì˜ˆ: Turn 3ì—ì„œ ë§ì´ ë¹¨ë¼ì§, ë‹µë³€ì´ ë‘ì„œì—†ìŒ)

            ## 4. Action Plan
            - ë‹¤ìŒ ë©´ì ‘ì„ ìœ„í•´ êµ¬ì²´ì ìœ¼ë¡œ ì—°ìŠµí•´ì•¼ í•  ì 
            """

            response = self.groq_client.chat.completions.create(
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": history_text},
                ],
                model="llama-3.3-70b-versatile",
                temperature=0.6,
                max_tokens=1000
            )

            return response.choices[0].message.content

        except Exception as e:
            print(f"[Report Error] {e}")
            return "ë¦¬í¬íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    # =========================================================================
    # TTS: í…ìŠ¤íŠ¸ â†’ ìŒì„± ë³€í™˜
    # =========================================================================
    def text_to_speech_stream(self, text: str):
        if not text or not isinstance(text, str) or len(text.strip()) == 0:
            return []
        try:
            audio_stream = self.tts_client.text_to_speech.convert(
                voice_id="JBFqnCBsd6RMkjVDRZzb",
                output_format="pcm_16000",
                text=text,
                model_id="eleven_turbo_v2_5"
            )
            return audio_stream
        except: return []
